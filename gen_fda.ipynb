{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw, Descriptors\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "#Import Keras objects\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Concatenate\n",
    "from keras import regularizers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FDA dataset: https://www.bindingdb.org/bind/ByFDAdrugs.jsp\n",
    "# RNN code: https://www.wildcardconsulting.dk/useful-information/master-your-molecule-generator-seq2seq-rnn-models-with-smiles-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_smi(smi):\n",
    "    return Chem.MolToSmiles((Chem.MolFromSmiles(smi)),isomericSmiles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('cns_drugs.txt',sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna(axis=0, how='any', thresh=None, subset=[\"smiles\"], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.rename(columns={'Ligand SMILES': 'smiles'})\n",
    "\n",
    "#data[\"smiles\"]=data[\"Ligand SMILES\"].apply(can_smi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import PandasTools\n",
    "PandasTools.AddMoleculeColumnToFrame(data,'smiles','Molecule',includeFingerprints=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors\n",
    "data['MolWt']=data['Molecule'].apply(Chem.Descriptors.MolWt)\n",
    "data['MolLogP']=data['Molecule'].apply(Chem.Descriptors.MolLogP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to clean the input data better ...\n",
    "data=data[data['MolWt']<500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['MolLogP']<6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_train, smiles_test = train_test_split(data[\"smiles\"], random_state=42)\n",
    "print smiles_train.shape\n",
    "print smiles_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = set(\"\".join(list(data.smiles))+\"!E\")\n",
    "char_to_int = dict((c,i) for i,c in enumerate(charset))\n",
    "int_to_char = dict((i,c) for i,c in enumerate(charset))\n",
    "embed = max([len(smile) for smile in data.smiles]) + 5\n",
    "print str(charset)\n",
    "print(len(charset), embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(smiles):\n",
    "        one_hot =  np.zeros((smiles.shape[0], embed , len(charset)),dtype=np.int8)\n",
    "        for i,smile in enumerate(smiles):\n",
    "            #encode the startchar\n",
    "            one_hot[i,0,char_to_int[\"!\"]] = 1\n",
    "            #encode the rest of the chars\n",
    "            for j,c in enumerate(smile):\n",
    "                one_hot[i,j+1,char_to_int[c]] = 1\n",
    "            #Encode endchar\n",
    "            one_hot[i,len(smile)+1:,char_to_int[\"E\"]] = 1\n",
    "        #Return two, one for input and the other for output\n",
    "        return one_hot[:,0:-1,:], one_hot[:,1:,:]\n",
    "X_train, Y_train = vectorize(smiles_train.values)\n",
    "X_test,Y_test = vectorize(smiles_test.values)\n",
    "print smiles_train.iloc[0]\n",
    "plt.matshow(X_train[0].T)\n",
    "#print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join([int_to_char[idx] for idx in np.argmax(X_train[0,:,:], axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "output_dim = Y_train.shape[-1]\n",
    "latent_dim = 64\n",
    "lstm_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unroll = False\n",
    "encoder_inputs = Input(shape=input_shape)\n",
    "encoder = LSTM(lstm_dim, return_state=True,\n",
    "                unroll=unroll)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "states = Concatenate(axis=-1)([state_h, state_c])\n",
    "neck = Dense(latent_dim, activation=\"relu\")\n",
    "neck_outputs = neck(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_h = Dense(lstm_dim, activation=\"relu\")\n",
    "decode_c = Dense(lstm_dim, activation=\"relu\")\n",
    "state_h_decoded =  decode_h(neck_outputs)\n",
    "state_c_decoded =  decode_c(neck_outputs)\n",
    "encoder_states = [state_h_decoded, state_c_decoded]\n",
    "decoder_inputs = Input(shape=input_shape)\n",
    "decoder_lstm = LSTM(lstm_dim,\n",
    "                    return_sequences=True,\n",
    "                    unroll=unroll\n",
    "                   )\n",
    "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_dim, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#Define the model, that inputs the training vector for two places, and predicts one character ahead of the input\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History, ReduceLROnPlateau\n",
    "h = History()\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=10, min_lr=0.000001, verbose=1, epsilon=1e-5)\n",
    "\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "opt=Adam(lr=0.005) #Default 0.001\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "\n",
    "model.fit([X_train,X_train],Y_train,\n",
    "                    epochs=100, # was 200\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[h, rlr],\n",
    "                    validation_data=[[X_test,X_test],Y_test ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(h.history, file(\"Blog_history.pickle\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h.history[\"loss\"], label=\"Loss\")\n",
    "plt.plot(h.history[\"val_loss\"], label=\"Val_Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    v = model.predict([X_test[i:i+1], X_test[i:i+1]]) #Can't be done as output not necessarely 1\n",
    "    idxs = np.argmax(v, axis=2)\n",
    "    pred=  \"\".join([int_to_char[h] for h in idxs[0]])[:-1]\n",
    "    idxs2 = np.argmax(X_test[i:i+1], axis=2)\n",
    "    true =  \"\".join([int_to_char[k] for k in idxs2[0]])[1:]\n",
    "    if true != pred:\n",
    "        print true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_to_latent_model = Model(encoder_inputs, neck_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_to_latent_model.save(\"Blog_simple_smi2lat.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_input = Input(shape=(latent_dim,))\n",
    "#reuse_layers\n",
    "state_h_decoded_2 =  decode_h(latent_input)\n",
    "state_c_decoded_2 =  decode_c(latent_input)\n",
    "latent_to_states_model = Model(latent_input, [state_h_decoded_2, state_c_decoded_2])\n",
    "latent_to_states_model.save(\"Blog_simple_lat2state.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last one is special, we need to change it to stateful, and change the input shape\n",
    "inf_decoder_inputs = Input(batch_shape=(1, 1, input_shape[1]))\n",
    "inf_decoder_lstm = LSTM(lstm_dim,\n",
    "                    return_sequences=True,\n",
    "                    unroll=unroll,\n",
    "                    stateful=True\n",
    "                   )\n",
    "inf_decoder_outputs = inf_decoder_lstm(inf_decoder_inputs)\n",
    "inf_decoder_dense = Dense(output_dim, activation='softmax')\n",
    "inf_decoder_outputs = inf_decoder_dense(inf_decoder_outputs)\n",
    "sample_model = Model(inf_decoder_inputs, inf_decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer Weights\n",
    "for i in range(1,3):\n",
    "    sample_model.layers[i].set_weights(model.layers[i+6].get_weights())\n",
    "sample_model.save(\"Blog_simple_samplemodel.h5\")\n",
    "\n",
    "sample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_latent = smiles_to_latent_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molno = 5\n",
    "latent_mol = smiles_to_latent_model.predict(X_test[molno:molno+1])\n",
    "sorti = np.argsort(np.sum(np.abs(x_latent - latent_mol), axis=1))\n",
    "print sorti[0:10]\n",
    "print smiles_test.iloc[sorti[0:8]]\n",
    "Draw.MolsToImage(smiles_test.iloc[sorti[0:8]].apply(Chem.MolFromSmiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Draw.MolsToImage(smiles_test.iloc[sorti[-8:]].apply(Chem.MolFromSmiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp = smiles_test.apply(Chem.MolFromSmiles).apply(Descriptors.MolLogP)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "red = pca.fit_transform(x_latent)\n",
    "plt.figure()\n",
    "plt.scatter(red[:,0], red[:,1],marker='.', c= logp)\n",
    "print(pca.explained_variance_ratio_, np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molwt = smiles_test.apply(Chem.MolFromSmiles).apply(Descriptors.MolMR)\n",
    "plt.figure()\n",
    "plt.scatter(red[:,0], red[:,1],marker='.', c= molwt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model LogP?\n",
    "x_train_latent = smiles_to_latent_model.predict(X_train)\n",
    "logp_train = smiles_train.apply(Chem.MolFromSmiles).apply(Descriptors.MolLogP)\n",
    "\n",
    "from keras.models import Sequential\n",
    "logp_model = Sequential()\n",
    "logp_model.add(Dense(128, input_shape=(latent_dim,), activation=\"relu\"))\n",
    "logp_model.add(Dense(128, activation=\"relu\"))\n",
    "logp_model.add(Dense(1))\n",
    "logp_model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,patience=10, min_lr=0.000001, verbose=1, epsilon=1e-5)\n",
    "logp_model.fit(x_train_latent, logp_train, batch_size=128, epochs=400, callbacks = [rlr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp_pred_train = logp_model.predict(x_train_latent)\n",
    "logp_pred_test = logp_model.predict(x_latent)\n",
    "plt.scatter(logp, logp_pred_test, label=\"Test\")\n",
    "plt.scatter(logp_train, logp_pred_train, label=\"Train\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_to_smiles(latent):\n",
    "    #decode states and set Reset the LSTM cells with them\n",
    "    states = latent_to_states_model.predict(latent)\n",
    "    sample_model.layers[1].reset_states(states=[states[0],states[1]])\n",
    "    #Prepare the input char\n",
    "    startidx = char_to_int[\"!\"]\n",
    "    samplevec = np.zeros((1,1,22))\n",
    "    samplevec[0,0,startidx] = 1\n",
    "    smiles = \"\"\n",
    "    #Loop and predict next char\n",
    "    for i in range(28):\n",
    "        o = sample_model.predict(samplevec)\n",
    "        sampleidx = np.argmax(o)\n",
    "        samplechar = int_to_char[sampleidx]\n",
    "        if samplechar != \"E\":\n",
    "            smiles = smiles + int_to_char[sampleidx]\n",
    "            samplevec = np.zeros((1,1,22))\n",
    "            samplevec[0,0,sampleidx] = 1\n",
    "        else:\n",
    "            break\n",
    "    return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = latent_to_smiles(x_latent[0:1])\n",
    "print smiles\n",
    "print smiles_test.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = 0\n",
    "for i in range(100):\n",
    "    smiles = latent_to_smiles(x_latent[i:i+1])\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        pass\n",
    "    else:\n",
    "        print smiles\n",
    "        wrong = wrong + 1\n",
    "print \"%0.1F percent wrongly formatted smiles\"%(wrong/float(1000)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolation test in latent_space\n",
    "i = 0\n",
    "j= 2\n",
    "latent1 = x_latent[j:j+1]\n",
    "latent0 = x_latent[i:i+1]\n",
    "mols1 = []\n",
    "ratios = np.linspace(0,1,25)\n",
    "for r in ratios:\n",
    "    #print r\n",
    "    rlatent = (1.0-r)*latent0 + r*latent1\n",
    "    smiles  = latent_to_smiles(rlatent)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        mols1.append(mol)\n",
    "    else:\n",
    "        print smiles\n",
    "         \n",
    "Draw.MolsToGridImage(mols1, molsPerRow=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample around the latent wector\n",
    "latent = x_latent[0:1]\n",
    "scale = 0.40\n",
    "mols = []\n",
    "for i in range(20):\n",
    "    latent_r = latent + scale*(np.random.randn(latent.shape[1])) #TODO, try with \n",
    "    smiles = latent_to_smiles(latent_r)\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        mols.append(mol)\n",
    "    else:\n",
    "        print smiles\n",
    "Draw.MolsToGridImage(mols, molsPerRow=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mdtraj]",
   "language": "python",
   "name": "conda-env-mdtraj-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
